{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install `LangChain` library"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please run the following script if you do not have the library LangChain installed in your computer. For more details, please visit [LangChain doumentation](https://python.langchain.com/en/latest/getting_started/getting_started.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install langchain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate an LLM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain supports a variety of LLM providers which can be found in [this link](https://python.langchain.com/en/latest/modules/models/llms/integrations.html). The model `google/flan-t5-xl` from [Hugging Face](https://huggingface.co/) will be used as the main LLM throughout the notebooks (please prepare [Hugging Face's API token](https://huggingface.co/docs/hub/security-tokens#:~:text=To%20create%20an%20access%20token,you're%20ready%20to%20go!) to access the model). However, you can change the model to other LLMs, such as [OpenAI](https://python.langchain.com/en/latest/modules/models/llms/integrations/openai.html), to obtain better experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain import HuggingFaceHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace with your token here.\n",
    "HUGGINGFACEHUB_API_TOKEN = os.environ.get(\"HUGGINGFACEHUB_API_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize LLM.\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id = \"google/flan-t5-xl\",\n",
    "    huggingfacehub_api_token = HUGGINGFACEHUB_API_TOKEN,\n",
    "    model_kwargs = {\n",
    "        \"temperature\": 0\n",
    "    }\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try it with your query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Je t'aime\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"Translate 'I love you' in French\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChain is a blockchain platform that allows users to create and manage their own private'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What is LangChain?\"\n",
    "llm(question)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What? LangChain is now a blockchain platform (LOL). Please note that language models are simply probability models that use the input as a condition to select what comes next based on the highest conditional probability. This issue may be attributed to the limited datasets used for training the model. However, we can improve the model performance by leveraging in-context learning, where a `prompt` is attached to guide the model to behave more reasonably in the desired task."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Empower the LLM with prompt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A possible solution is to provide the model with additional context or knowledge in the form of a prompt. This allows the model to incorporate the new information into its generation process which hopefully improves the quality of its answers.\n",
    "\n",
    "For implementation, LangChain provides `PromptTemplate` which is a powerful tool to manage a prompt. Here is an example of the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the [PromptTemplate](https://python.langchain.com/en/latest/modules/prompts/prompt_templates/getting_started.html), it requires two essential ingredients to create a prompt. These are `template` and `input_variables`.\n",
    "- `template` is a text used to provide additional information to the model, such as instructions and few-shot example. It may also contain placeholders for flexibility, allowing for the creation of prompts associated with specific questions or tasks.\n",
    "- `input_variables` is a list of variables to replace with the placeholders used in the `template`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Given the following extracted parts of a long document and a question, create a final answer. \n",
    "If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
    "QUESTION: {question}\n",
    "=========\n",
    "{context}\n",
    "=========\n",
    "FINAL ANSWER: \n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this examples, `template` contains two placeholders which are `question` and `context`, meaning that `input_variables` is `[\"question\", \"context\"]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = PromptTemplate(\n",
    "    template = template,\n",
    "    input_variables = [\"question\", \"context\"],\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can attach the prompt to LLM using `LLMChain` by specifying three main inputs, namely\n",
    "- `llm`: a large language model we would like to deploy.\n",
    "- `prompt`: a `PromptTemplate`.\n",
    "- `output_key`: a key in the dictionary to represent the output text (the output of `LLMChain` is designed as a dictionary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(\n",
    "    llm = llm,\n",
    "    prompt = PROMPT,\n",
    "    output_key = \"answer\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate an answer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `PromptTemplate`, the variable `context` can be used to provide additional context to the model. In this case, we may create context for the LangChain here since it is relevant to the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "LangChain is a framework for developing applications powered by language models. \n",
    "We believe that the most powerful and differentiated applications will not only call out \n",
    "to a language model via an API, but will also:\n",
    "- Be data-aware: connect a language model to other sources of data\n",
    "- Be agentic: allow a language model to interact with its environment\n",
    "The LangChain framework is designed with the above principles in mind.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is LangChain?\n",
      "A: a framework for developing applications powered by language models\n"
     ]
    }
   ],
   "source": [
    "output = llm_chain(\n",
    "    {\n",
    "        \"question\": question,\n",
    "        \"context\": context\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Q:\", question)\n",
    "print(\"A:\", output[\"answer\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are! The model is now able to generate the answer more accurately with the additional context provided."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
